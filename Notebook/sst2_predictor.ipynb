{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b7dfe6-4131-410c-ba45-c7d993ecafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, GenerationConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training, AdaLoraConfig, AdaLoraConfig\n",
    "\n",
    "from transformers import TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab4452c-2d62-48fd-ba16-58af84450252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets  = load_dataset(\"glue\", 'sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55348e23-57d6-461a-84a4-8d9445503dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a41c216645443db5cf655d379fcd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label', 'idx', 'output', 'input', 'labels'],\n",
      "    num_rows: 872\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    #print(data_point)\n",
    "    # sorry about the formatting disaster gotta move fast\n",
    "    return f\"\"\"Below is an input that describes a task. Classify the sentiment of this input as positive or negative.\n",
    "                ### Input: {data_point}\"\"\" # noqa: E501\n",
    "\n",
    "\n",
    "# Assuming 'dataset' is your original DatasetDict object\n",
    "def add_textual_sentiment(example):\n",
    "    if example['label'] == 1:\n",
    "        example['output'] = 'positive'\n",
    "    else:\n",
    "        example['output'] = 'negative'\n",
    "    example['input'] = generate_prompt(example['sentence']) \n",
    "    example['labels'] = example['label'] \n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Apply the function to add the 'output' column\n",
    "test_data = raw_datasets['validation'].map(add_textual_sentiment)\n",
    "\n",
    "# Now you can inspect your dataset\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948ca3ae-1657-403b-ad98-75714c0e6f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e7a145a8844bb2a88914473312e0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = raw_datasets['train'].map(add_textual_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ff8dfd-0eb2-417f-ac41-5d83650122fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/kowsher/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# Log in using your Hugging Face token\n",
    "login(\"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\")\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bde7955-c697-45e0-af25-df59112ffd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16282c68b98d41888ad46986a97bc4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example):\n",
    "    \"\"\"\n",
    "    Tokenize a single example using the specified tokenizer.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        example['input'],\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "# Assuming train_data is a dataset-like object, map the tokenize function to it.\n",
    "train_data = train_data.map(tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e67d1d-add4-4156-bd50-70b5e0a2fac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5fa67c8-3433-4b6a-a805-0ef5d2e06ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b4dd7283f74166883173807b0e9ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = test_data.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d93d4b9-0915-4f47-b812-9b725fb91b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at HuggingFaceTB/SmolLM-135M-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "     num_labels=2\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bae6d09-94bb-43a8-aa3b-9d99b19a7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leader\n",
    "\n",
    "leader.PEFT(model, method='column', rank=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "963c1b1a-ec25-4888-b799-aefe7fc612cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fea91c00-d4d4-4633-815e-cd86aad4e7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowsher/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/kowsher/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/kowsher/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-24 11:54:01,635] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kowsher/miniconda3/envs/LD/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/kowsher/miniconda3/envs/LD/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahmedshuvo969\u001b[0m (\u001b[33mprojectstevens\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kowsher/investigation/wandb/run-20241224_115402-81krypgi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/projectstevens/huggingface/runs/81krypgi' target=\"_blank\">dir</a></strong> to <a href='https://wandb.ai/projectstevens/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/projectstevens/huggingface' target=\"_blank\">https://wandb.ai/projectstevens/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/projectstevens/huggingface/runs/81krypgi' target=\"_blank\">https://wandb.ai/projectstevens/huggingface/runs/81krypgi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13065' max='56125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13065/56125 34:46 < 1:54:38, 6.26 it/s, Epoch 1.16/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.344101</td>\n",
       "      <td>0.856852</td>\n",
       "      <td>0.852278</td>\n",
       "      <td>0.852559</td>\n",
       "      <td>0.853211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.438700</td>\n",
       "      <td>0.570161</td>\n",
       "      <td>0.841399</td>\n",
       "      <td>0.818978</td>\n",
       "      <td>0.813866</td>\n",
       "      <td>0.816514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.385367</td>\n",
       "      <td>0.845892</td>\n",
       "      <td>0.840701</td>\n",
       "      <td>0.839002</td>\n",
       "      <td>0.839450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>0.450282</td>\n",
       "      <td>0.841744</td>\n",
       "      <td>0.833544</td>\n",
       "      <td>0.833598</td>\n",
       "      <td>0.834862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.559114</td>\n",
       "      <td>0.840516</td>\n",
       "      <td>0.801381</td>\n",
       "      <td>0.792825</td>\n",
       "      <td>0.798165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.345053</td>\n",
       "      <td>0.863997</td>\n",
       "      <td>0.861055</td>\n",
       "      <td>0.859903</td>\n",
       "      <td>0.860092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.367900</td>\n",
       "      <td>0.364870</td>\n",
       "      <td>0.850659</td>\n",
       "      <td>0.849331</td>\n",
       "      <td>0.848544</td>\n",
       "      <td>0.848624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.378300</td>\n",
       "      <td>0.356900</td>\n",
       "      <td>0.843307</td>\n",
       "      <td>0.839764</td>\n",
       "      <td>0.840005</td>\n",
       "      <td>0.840596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.359603</td>\n",
       "      <td>0.863479</td>\n",
       "      <td>0.863507</td>\n",
       "      <td>0.863492</td>\n",
       "      <td>0.863532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.449849</td>\n",
       "      <td>0.846919</td>\n",
       "      <td>0.838090</td>\n",
       "      <td>0.838155</td>\n",
       "      <td>0.839450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.428708</td>\n",
       "      <td>0.853016</td>\n",
       "      <td>0.851572</td>\n",
       "      <td>0.851797</td>\n",
       "      <td>0.852064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>0.431492</td>\n",
       "      <td>0.834025</td>\n",
       "      <td>0.821735</td>\n",
       "      <td>0.821407</td>\n",
       "      <td>0.823394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.348500</td>\n",
       "      <td>0.495175</td>\n",
       "      <td>0.820876</td>\n",
       "      <td>0.805422</td>\n",
       "      <td>0.804559</td>\n",
       "      <td>0.807339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.337600</td>\n",
       "      <td>0.386911</td>\n",
       "      <td>0.849730</td>\n",
       "      <td>0.849699</td>\n",
       "      <td>0.849714</td>\n",
       "      <td>0.849771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.343600</td>\n",
       "      <td>0.418761</td>\n",
       "      <td>0.842592</td>\n",
       "      <td>0.838554</td>\n",
       "      <td>0.838785</td>\n",
       "      <td>0.839450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.326900</td>\n",
       "      <td>0.392088</td>\n",
       "      <td>0.850645</td>\n",
       "      <td>0.843205</td>\n",
       "      <td>0.841088</td>\n",
       "      <td>0.841743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.478423</td>\n",
       "      <td>0.825369</td>\n",
       "      <td>0.823135</td>\n",
       "      <td>0.822051</td>\n",
       "      <td>0.822248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.398117</td>\n",
       "      <td>0.860112</td>\n",
       "      <td>0.859960</td>\n",
       "      <td>0.860018</td>\n",
       "      <td>0.860092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.308300</td>\n",
       "      <td>0.409174</td>\n",
       "      <td>0.824624</td>\n",
       "      <td>0.820115</td>\n",
       "      <td>0.820250</td>\n",
       "      <td>0.821101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.393760</td>\n",
       "      <td>0.843976</td>\n",
       "      <td>0.844026</td>\n",
       "      <td>0.843996</td>\n",
       "      <td>0.844037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.319400</td>\n",
       "      <td>0.470379</td>\n",
       "      <td>0.837121</td>\n",
       "      <td>0.837227</td>\n",
       "      <td>0.837135</td>\n",
       "      <td>0.837156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.331300</td>\n",
       "      <td>0.436503</td>\n",
       "      <td>0.837235</td>\n",
       "      <td>0.834649</td>\n",
       "      <td>0.833505</td>\n",
       "      <td>0.833716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.298700</td>\n",
       "      <td>0.406691</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.854456</td>\n",
       "      <td>0.854342</td>\n",
       "      <td>0.854358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.315300</td>\n",
       "      <td>0.415911</td>\n",
       "      <td>0.832954</td>\n",
       "      <td>0.832218</td>\n",
       "      <td>0.832370</td>\n",
       "      <td>0.832569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.319200</td>\n",
       "      <td>0.527694</td>\n",
       "      <td>0.843191</td>\n",
       "      <td>0.833418</td>\n",
       "      <td>0.833389</td>\n",
       "      <td>0.834862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.403223</td>\n",
       "      <td>0.821035</td>\n",
       "      <td>0.821083</td>\n",
       "      <td>0.821055</td>\n",
       "      <td>0.821101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.417297</td>\n",
       "      <td>0.846559</td>\n",
       "      <td>0.841932</td>\n",
       "      <td>0.842167</td>\n",
       "      <td>0.842890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.314400</td>\n",
       "      <td>0.519325</td>\n",
       "      <td>0.836940</td>\n",
       "      <td>0.832797</td>\n",
       "      <td>0.833002</td>\n",
       "      <td>0.833716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.411521</td>\n",
       "      <td>0.844428</td>\n",
       "      <td>0.844363</td>\n",
       "      <td>0.844036</td>\n",
       "      <td>0.844037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.297800</td>\n",
       "      <td>0.493710</td>\n",
       "      <td>0.834983</td>\n",
       "      <td>0.829166</td>\n",
       "      <td>0.829297</td>\n",
       "      <td>0.830275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.301800</td>\n",
       "      <td>0.510198</td>\n",
       "      <td>0.836990</td>\n",
       "      <td>0.823988</td>\n",
       "      <td>0.823639</td>\n",
       "      <td>0.825688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.287400</td>\n",
       "      <td>0.392278</td>\n",
       "      <td>0.852292</td>\n",
       "      <td>0.848983</td>\n",
       "      <td>0.849255</td>\n",
       "      <td>0.849771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.345200</td>\n",
       "      <td>0.361744</td>\n",
       "      <td>0.847815</td>\n",
       "      <td>0.847784</td>\n",
       "      <td>0.847477</td>\n",
       "      <td>0.847477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.311800</td>\n",
       "      <td>0.387889</td>\n",
       "      <td>0.862846</td>\n",
       "      <td>0.862044</td>\n",
       "      <td>0.862222</td>\n",
       "      <td>0.862385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.460790</td>\n",
       "      <td>0.829151</td>\n",
       "      <td>0.829260</td>\n",
       "      <td>0.829117</td>\n",
       "      <td>0.829128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.518868</td>\n",
       "      <td>0.845448</td>\n",
       "      <td>0.825903</td>\n",
       "      <td>0.825096</td>\n",
       "      <td>0.827982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.374820</td>\n",
       "      <td>0.863383</td>\n",
       "      <td>0.862886</td>\n",
       "      <td>0.862367</td>\n",
       "      <td>0.862385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.295300</td>\n",
       "      <td>0.340127</td>\n",
       "      <td>0.863486</td>\n",
       "      <td>0.863591</td>\n",
       "      <td>0.863510</td>\n",
       "      <td>0.863532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.309700</td>\n",
       "      <td>0.376678</td>\n",
       "      <td>0.850892</td>\n",
       "      <td>0.849236</td>\n",
       "      <td>0.849470</td>\n",
       "      <td>0.849771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.299400</td>\n",
       "      <td>0.357938</td>\n",
       "      <td>0.871648</td>\n",
       "      <td>0.871390</td>\n",
       "      <td>0.871478</td>\n",
       "      <td>0.871560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.300700</td>\n",
       "      <td>0.367652</td>\n",
       "      <td>0.867581</td>\n",
       "      <td>0.866591</td>\n",
       "      <td>0.866793</td>\n",
       "      <td>0.866972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.303800</td>\n",
       "      <td>0.352319</td>\n",
       "      <td>0.870961</td>\n",
       "      <td>0.870053</td>\n",
       "      <td>0.870249</td>\n",
       "      <td>0.870413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.427920</td>\n",
       "      <td>0.860206</td>\n",
       "      <td>0.860297</td>\n",
       "      <td>0.860089</td>\n",
       "      <td>0.860092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.437261</td>\n",
       "      <td>0.857361</td>\n",
       "      <td>0.847184</td>\n",
       "      <td>0.847273</td>\n",
       "      <td>0.848624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.318416</td>\n",
       "      <td>0.875007</td>\n",
       "      <td>0.874895</td>\n",
       "      <td>0.874941</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.328797</td>\n",
       "      <td>0.872130</td>\n",
       "      <td>0.868464</td>\n",
       "      <td>0.868799</td>\n",
       "      <td>0.869266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.361407</td>\n",
       "      <td>0.877475</td>\n",
       "      <td>0.874263</td>\n",
       "      <td>0.874604</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.322600</td>\n",
       "      <td>0.329198</td>\n",
       "      <td>0.864622</td>\n",
       "      <td>0.864675</td>\n",
       "      <td>0.864644</td>\n",
       "      <td>0.864679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.444483</td>\n",
       "      <td>0.860889</td>\n",
       "      <td>0.856635</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.855505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.292600</td>\n",
       "      <td>0.356350</td>\n",
       "      <td>0.875349</td>\n",
       "      <td>0.875316</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.294400</td>\n",
       "      <td>0.380917</td>\n",
       "      <td>0.863457</td>\n",
       "      <td>0.861971</td>\n",
       "      <td>0.861158</td>\n",
       "      <td>0.861239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.399166</td>\n",
       "      <td>0.863486</td>\n",
       "      <td>0.863591</td>\n",
       "      <td>0.863510</td>\n",
       "      <td>0.863532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>0.419161</td>\n",
       "      <td>0.864485</td>\n",
       "      <td>0.860371</td>\n",
       "      <td>0.860684</td>\n",
       "      <td>0.861239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.294900</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.865769</td>\n",
       "      <td>0.865844</td>\n",
       "      <td>0.865796</td>\n",
       "      <td>0.865826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.286600</td>\n",
       "      <td>0.414444</td>\n",
       "      <td>0.858963</td>\n",
       "      <td>0.855908</td>\n",
       "      <td>0.856197</td>\n",
       "      <td>0.856651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.279100</td>\n",
       "      <td>0.359003</td>\n",
       "      <td>0.871107</td>\n",
       "      <td>0.870011</td>\n",
       "      <td>0.870227</td>\n",
       "      <td>0.870413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.241800</td>\n",
       "      <td>0.430425</td>\n",
       "      <td>0.864815</td>\n",
       "      <td>0.860329</td>\n",
       "      <td>0.860643</td>\n",
       "      <td>0.861239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.253600</td>\n",
       "      <td>0.430112</td>\n",
       "      <td>0.856398</td>\n",
       "      <td>0.854088</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>0.853211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.293200</td>\n",
       "      <td>0.333591</td>\n",
       "      <td>0.867646</td>\n",
       "      <td>0.867391</td>\n",
       "      <td>0.866966</td>\n",
       "      <td>0.866972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.243700</td>\n",
       "      <td>0.365573</td>\n",
       "      <td>0.873944</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.873773</td>\n",
       "      <td>0.873853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.260400</td>\n",
       "      <td>0.432635</td>\n",
       "      <td>0.866919</td>\n",
       "      <td>0.867012</td>\n",
       "      <td>0.866947</td>\n",
       "      <td>0.866972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>0.421346</td>\n",
       "      <td>0.859601</td>\n",
       "      <td>0.858540</td>\n",
       "      <td>0.858743</td>\n",
       "      <td>0.858945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.440717</td>\n",
       "      <td>0.865792</td>\n",
       "      <td>0.865759</td>\n",
       "      <td>0.865775</td>\n",
       "      <td>0.865826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.254200</td>\n",
       "      <td>0.399573</td>\n",
       "      <td>0.871447</td>\n",
       "      <td>0.867254</td>\n",
       "      <td>0.867593</td>\n",
       "      <td>0.868119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>0.384063</td>\n",
       "      <td>0.869293</td>\n",
       "      <td>0.869138</td>\n",
       "      <td>0.869197</td>\n",
       "      <td>0.869266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='436' max='436' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [436/436 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m\n\u001b[1;32m     26\u001b[0m compute_metrics \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mClassificationMetrics()\n\u001b[1;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     29\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/LD/lib/python3.10/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/LD/lib/python3.10/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LD/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LD/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LD/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from generator import metrics\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='dir',\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=3,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.00,\n",
    "    evaluation_strategy=\"steps\",\n",
    "  \n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=200,\n",
    "\n",
    "    \n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "compute_metrics = metrics.ClassificationMetrics()\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    "    data_collator=data_collator,\n",
    "    \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69139853-b2b1-444a-9b59-06a4b0bd04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred, labels, mat = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d9bc64c-ccfd-49a4-a75d-03c36704d846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.39027222990989685,\n",
       " 'test_precision': 0.8701011378002529,\n",
       " 'test_recall': 0.8697272038393533,\n",
       " 'test_f1-score': 0.8692550505050505,\n",
       " 'test_accuracy': 0.8692660550458715,\n",
       " 'test_runtime': 5.5679,\n",
       " 'test_samples_per_second': 156.612,\n",
       " 'test_steps_per_second': 78.306}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dcdf8b-b3fb-4a5f-a8a4-398d8e811555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
