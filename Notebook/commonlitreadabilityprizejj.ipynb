{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f00ef4-41c6-430f-b3d2-a54e52788075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in assessing text readability. Your task is to analyze the given passage and predict its readability score on a continuous scale. The readability score reflects how easy or difficult the text is to understand. \n",
      "\n",
      "    ### Passage:\n",
      "    On the way Mother Mitchel arranged in her head the plan of the monument which was to immortalize her, and considered the means of executing it. As to its form and size, it was to be as exact a copy of the capitol as possible, since the King had willed it; but its outside crust should have a beauty all its own. The dome must be adorned with sugarplums of all colors, and surmounted by a splendid crown of macaroons, spun sugar, chocolate, and candied fruits. It was no small affair.\n",
      "Mother Mitchel did not like to lose her time. Her plan of battle once formed, she recruited on her way all the little pastry cooks of the country, as well as all the tiny six-year-olds who had a sincere love for the noble callings of scullion and apprentice. There were plenty of these, as you may suppose, in the country of the Greedy; Mother Mitchel had her pick of them.\n",
      "\n",
      "    ### Response: The predicted readability score for the passage is  -1.101295212 -1.101295212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb4e29a17da4497b3c046fb0224e403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4725' max='4725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4725/4725 9:30:56, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>R2</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearman's rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>92.489300</td>\n",
       "      <td>18.102417</td>\n",
       "      <td>490096001.804101</td>\n",
       "      <td>3593664561734716416.000000</td>\n",
       "      <td>1895696326.349428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3590464208627334144.000000</td>\n",
       "      <td>0.036869</td>\n",
       "      <td>-0.012833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>57.913400</td>\n",
       "      <td>12.733817</td>\n",
       "      <td>0.683047</td>\n",
       "      <td>0.725595</td>\n",
       "      <td>0.851819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275051</td>\n",
       "      <td>0.614759</td>\n",
       "      <td>0.594110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>50.389100</td>\n",
       "      <td>12.466120</td>\n",
       "      <td>0.748113</td>\n",
       "      <td>0.865066</td>\n",
       "      <td>0.930089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135705</td>\n",
       "      <td>0.676080</td>\n",
       "      <td>0.754990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>49.335100</td>\n",
       "      <td>12.486561</td>\n",
       "      <td>0.545443</td>\n",
       "      <td>0.463665</td>\n",
       "      <td>0.680929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.536748</td>\n",
       "      <td>0.764662</td>\n",
       "      <td>0.785730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>47.981700</td>\n",
       "      <td>12.501547</td>\n",
       "      <td>0.555874</td>\n",
       "      <td>0.490086</td>\n",
       "      <td>0.700062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510350</td>\n",
       "      <td>0.761488</td>\n",
       "      <td>0.792368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>47.015500</td>\n",
       "      <td>12.877935</td>\n",
       "      <td>0.553160</td>\n",
       "      <td>0.473622</td>\n",
       "      <td>0.688202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526800</td>\n",
       "      <td>0.761226</td>\n",
       "      <td>0.818093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>45.379400</td>\n",
       "      <td>12.761484</td>\n",
       "      <td>0.518225</td>\n",
       "      <td>0.421363</td>\n",
       "      <td>0.649125</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.579012</td>\n",
       "      <td>0.781905</td>\n",
       "      <td>0.833745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>44.126800</td>\n",
       "      <td>13.257968</td>\n",
       "      <td>0.507397</td>\n",
       "      <td>0.389388</td>\n",
       "      <td>0.624010</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.610959</td>\n",
       "      <td>0.794851</td>\n",
       "      <td>0.799047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>42.981200</td>\n",
       "      <td>13.233828</td>\n",
       "      <td>0.460672</td>\n",
       "      <td>0.331273</td>\n",
       "      <td>0.575564</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.669022</td>\n",
       "      <td>0.825583</td>\n",
       "      <td>0.842948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>41.248900</td>\n",
       "      <td>13.836454</td>\n",
       "      <td>0.558726</td>\n",
       "      <td>0.474364</td>\n",
       "      <td>0.688741</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.526059</td>\n",
       "      <td>0.784918</td>\n",
       "      <td>0.826207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>40.055800</td>\n",
       "      <td>13.739690</td>\n",
       "      <td>0.499620</td>\n",
       "      <td>0.383230</td>\n",
       "      <td>0.619056</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.617111</td>\n",
       "      <td>0.824033</td>\n",
       "      <td>0.826597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>37.749000</td>\n",
       "      <td>14.491285</td>\n",
       "      <td>0.440887</td>\n",
       "      <td>0.305312</td>\n",
       "      <td>0.552550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.694960</td>\n",
       "      <td>0.840537</td>\n",
       "      <td>0.845672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>37.573200</td>\n",
       "      <td>14.521331</td>\n",
       "      <td>0.446077</td>\n",
       "      <td>0.318824</td>\n",
       "      <td>0.564645</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.681460</td>\n",
       "      <td>0.839570</td>\n",
       "      <td>0.851460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>34.708700</td>\n",
       "      <td>15.152466</td>\n",
       "      <td>0.448534</td>\n",
       "      <td>0.322099</td>\n",
       "      <td>0.567538</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.678188</td>\n",
       "      <td>0.830883</td>\n",
       "      <td>0.843601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>34.534700</td>\n",
       "      <td>15.295631</td>\n",
       "      <td>0.447566</td>\n",
       "      <td>0.309514</td>\n",
       "      <td>0.556339</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.690762</td>\n",
       "      <td>0.831571</td>\n",
       "      <td>0.851078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>31.491600</td>\n",
       "      <td>16.360495</td>\n",
       "      <td>0.432141</td>\n",
       "      <td>0.303057</td>\n",
       "      <td>0.550506</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.697213</td>\n",
       "      <td>0.855980</td>\n",
       "      <td>0.863977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>32.196100</td>\n",
       "      <td>15.852289</td>\n",
       "      <td>0.461368</td>\n",
       "      <td>0.334015</td>\n",
       "      <td>0.577940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666283</td>\n",
       "      <td>0.828240</td>\n",
       "      <td>0.858852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>28.995900</td>\n",
       "      <td>16.407661</td>\n",
       "      <td>0.413446</td>\n",
       "      <td>0.273690</td>\n",
       "      <td>0.523153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.726554</td>\n",
       "      <td>0.853399</td>\n",
       "      <td>0.865036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>29.471000</td>\n",
       "      <td>17.917500</td>\n",
       "      <td>0.420741</td>\n",
       "      <td>0.279454</td>\n",
       "      <td>0.528634</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.720794</td>\n",
       "      <td>0.852872</td>\n",
       "      <td>0.858858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>27.225500</td>\n",
       "      <td>17.837385</td>\n",
       "      <td>0.414600</td>\n",
       "      <td>0.274663</td>\n",
       "      <td>0.524083</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.725582</td>\n",
       "      <td>0.853103</td>\n",
       "      <td>0.863580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>27.401100</td>\n",
       "      <td>19.021049</td>\n",
       "      <td>0.419644</td>\n",
       "      <td>0.276912</td>\n",
       "      <td>0.526224</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.723335</td>\n",
       "      <td>0.852985</td>\n",
       "      <td>0.860641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>25.702300</td>\n",
       "      <td>18.807373</td>\n",
       "      <td>0.413270</td>\n",
       "      <td>0.271602</td>\n",
       "      <td>0.521154</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.728640</td>\n",
       "      <td>0.853980</td>\n",
       "      <td>0.859039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>25.349300</td>\n",
       "      <td>19.826839</td>\n",
       "      <td>0.415443</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>0.518622</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.731271</td>\n",
       "      <td>0.856884</td>\n",
       "      <td>0.860289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>23.942600</td>\n",
       "      <td>19.737381</td>\n",
       "      <td>0.415533</td>\n",
       "      <td>0.278420</td>\n",
       "      <td>0.527656</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.721828</td>\n",
       "      <td>0.853140</td>\n",
       "      <td>0.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>23.336900</td>\n",
       "      <td>20.505087</td>\n",
       "      <td>0.411091</td>\n",
       "      <td>0.274278</td>\n",
       "      <td>0.523715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.725967</td>\n",
       "      <td>0.853235</td>\n",
       "      <td>0.862773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>22.225200</td>\n",
       "      <td>20.568108</td>\n",
       "      <td>0.416976</td>\n",
       "      <td>0.276664</td>\n",
       "      <td>0.525989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.723582</td>\n",
       "      <td>0.851049</td>\n",
       "      <td>0.855122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>20.861100</td>\n",
       "      <td>22.207476</td>\n",
       "      <td>0.413967</td>\n",
       "      <td>0.272359</td>\n",
       "      <td>0.521880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.727884</td>\n",
       "      <td>0.853411</td>\n",
       "      <td>0.859479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>19.939600</td>\n",
       "      <td>22.601818</td>\n",
       "      <td>0.419571</td>\n",
       "      <td>0.286920</td>\n",
       "      <td>0.535650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.713335</td>\n",
       "      <td>0.846668</td>\n",
       "      <td>0.848802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>18.007100</td>\n",
       "      <td>23.783623</td>\n",
       "      <td>0.422043</td>\n",
       "      <td>0.290116</td>\n",
       "      <td>0.538624</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.710142</td>\n",
       "      <td>0.843693</td>\n",
       "      <td>0.852448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>16.962100</td>\n",
       "      <td>24.157074</td>\n",
       "      <td>0.421313</td>\n",
       "      <td>0.286048</td>\n",
       "      <td>0.534834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714207</td>\n",
       "      <td>0.845362</td>\n",
       "      <td>0.852062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>14.208000</td>\n",
       "      <td>27.050848</td>\n",
       "      <td>0.446666</td>\n",
       "      <td>0.646976</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>0.672092</td>\n",
       "      <td>0.850030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>13.595900</td>\n",
       "      <td>27.324587</td>\n",
       "      <td>0.425113</td>\n",
       "      <td>0.290371</td>\n",
       "      <td>0.538861</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.709888</td>\n",
       "      <td>0.842755</td>\n",
       "      <td>0.848840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>11.007500</td>\n",
       "      <td>30.467934</td>\n",
       "      <td>27461.405071</td>\n",
       "      <td>427577896184.144043</td>\n",
       "      <td>653894.407519</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>-427197114887.284729</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.850320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>10.662600</td>\n",
       "      <td>30.009066</td>\n",
       "      <td>5293.039622</td>\n",
       "      <td>15882640376.752171</td>\n",
       "      <td>126026.347947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15868496023.487238</td>\n",
       "      <td>0.024369</td>\n",
       "      <td>0.844857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>8.611300</td>\n",
       "      <td>32.643387</td>\n",
       "      <td>27474.454885</td>\n",
       "      <td>427984415324.412659</td>\n",
       "      <td>654205.178308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-427603272000.221252</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.848682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>8.676100</td>\n",
       "      <td>32.638416</td>\n",
       "      <td>57.649292</td>\n",
       "      <td>1854997.731219</td>\n",
       "      <td>1361.983014</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>-1853344.755178</td>\n",
       "      <td>0.073943</td>\n",
       "      <td>0.844870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>7.750500</td>\n",
       "      <td>33.444458</td>\n",
       "      <td>0.430960</td>\n",
       "      <td>0.293068</td>\n",
       "      <td>0.541358</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.707193</td>\n",
       "      <td>0.841569</td>\n",
       "      <td>0.847891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>7.496600</td>\n",
       "      <td>34.060230</td>\n",
       "      <td>469.511217</td>\n",
       "      <td>124756127.319721</td>\n",
       "      <td>11169.428245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-124645024.225223</td>\n",
       "      <td>0.008177</td>\n",
       "      <td>0.832513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>7.059900</td>\n",
       "      <td>33.952690</td>\n",
       "      <td>34.287286</td>\n",
       "      <td>649762.601245</td>\n",
       "      <td>806.078533</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>-649182.952424</td>\n",
       "      <td>0.034976</td>\n",
       "      <td>0.841056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>6.943700</td>\n",
       "      <td>33.906483</td>\n",
       "      <td>0.436410</td>\n",
       "      <td>0.305246</td>\n",
       "      <td>0.552490</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.695026</td>\n",
       "      <td>0.833755</td>\n",
       "      <td>0.838981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>6.722100</td>\n",
       "      <td>33.533897</td>\n",
       "      <td>27510.092754</td>\n",
       "      <td>429095085987.752930</td>\n",
       "      <td>655053.498569</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>-428712953551.125549</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.838019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>6.619300</td>\n",
       "      <td>33.445038</td>\n",
       "      <td>2.243020</td>\n",
       "      <td>1839.502592</td>\n",
       "      <td>42.889423</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>-1836.864415</td>\n",
       "      <td>0.048245</td>\n",
       "      <td>0.832143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>6.497400</td>\n",
       "      <td>33.741261</td>\n",
       "      <td>2.244421</td>\n",
       "      <td>1839.505746</td>\n",
       "      <td>42.889460</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>-1836.867567</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.828649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>6.442500</td>\n",
       "      <td>33.828125</td>\n",
       "      <td>0.449662</td>\n",
       "      <td>0.322310</td>\n",
       "      <td>0.567723</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.677977</td>\n",
       "      <td>0.823424</td>\n",
       "      <td>0.823859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>6.379800</td>\n",
       "      <td>33.953358</td>\n",
       "      <td>0.448344</td>\n",
       "      <td>0.320589</td>\n",
       "      <td>0.566206</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.679696</td>\n",
       "      <td>0.824480</td>\n",
       "      <td>0.825143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>6.336400</td>\n",
       "      <td>33.922710</td>\n",
       "      <td>0.450497</td>\n",
       "      <td>0.323415</td>\n",
       "      <td>0.568696</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.676873</td>\n",
       "      <td>0.822748</td>\n",
       "      <td>0.823130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>6.332900</td>\n",
       "      <td>33.945030</td>\n",
       "      <td>0.450161</td>\n",
       "      <td>0.322876</td>\n",
       "      <td>0.568222</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.677411</td>\n",
       "      <td>0.823067</td>\n",
       "      <td>0.823634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at dir/checkpoint-300/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4725, training_loss=25.809901833509013, metrics={'train_runtime': 34259.4946, 'train_samples_per_second': 1.654, 'train_steps_per_second': 0.138, 'total_flos': 507576295300608.0, 'train_loss': 25.809901833509013, 'epoch': 25.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file = \"commonlitreadabilityprize.csv\"\n",
    "\n",
    "# Load the CSV file into a Hugging Face dataset\n",
    "dataset = load_dataset(\"csv\", data_files=csv_file)\n",
    "\n",
    "\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Access the train and test datasets\n",
    "train_data = train_test_split['train']\n",
    "val_data = train_test_split['test']\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# Log in using your Hugging Face token\n",
    "login(\"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\")\n",
    "\n",
    "model_name ='meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.model_max_length = 1000\n",
    "\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    # Print the data point for debugging (optional)\n",
    "    # print(data_point)\n",
    "    \n",
    "    return f\"\"\"You are an expert in assessing text readability. Your task is to analyze the given passage and predict its readability score on a continuous scale. The readability score reflects how easy or difficult the text is to understand. \n",
    "\n",
    "    ### Passage:\n",
    "    {data_point['excerpt']}\n",
    "\n",
    "    ### Response: The predicted readability score for the passage is \"\"\"\n",
    "\n",
    "# Assuming `dataset` is your DatasetDict\n",
    "def add_label_column(example):\n",
    "    total_length = 19\n",
    "    num = float(example['target'])\n",
    "\n",
    "    #formatted_num = str(num) + tokenizer.pad_token * padding_length\n",
    "\n",
    "    # Add labels and outputs to the example\n",
    "    example['labels'] = float(example['target'])\n",
    "    example['output'] = str(num)\n",
    "    example['input'] = generate_prompt(example)\n",
    "\n",
    "    \n",
    "    return example\n",
    "\n",
    "train_data = train_data.map(add_label_column)\n",
    "val_data = val_data.map(add_label_column)\n",
    "\n",
    "\n",
    "\n",
    "print(train_data['input'][2], train_data['labels'][2], train_data['output'][2])\n",
    "\n",
    "from generator.modeling import PredictorCausalLM\n",
    "from generator.collator import DataCollator\n",
    "from generator import metrics\n",
    "from generator.training import GenTrainer\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)  # Load configuration\n",
    "config.dense_representation = 10 \n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "config.nub_of_token_generation = 59\n",
    "\n",
    "\n",
    "model = PredictorCausalLM(config, num_labels=1)  # Instantiate model\n",
    "\n",
    "\n",
    "\n",
    "import leader\n",
    "\n",
    "leader.PEFT(model, method='column', rank=3)\n",
    "\n",
    "\n",
    "data_collator = DataCollator(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='dir',\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.00,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=100,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=200,\n",
    ")\n",
    "\n",
    "compute_metrics = metrics.RegressionMetrics(tokenizer)\n",
    "trainer = GenTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    max_steps_for_sampling=500,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383410e-2b16-4a0a-a578-2d50119abb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
