{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f00ef4-41c6-430f-b3d2-a54e52788075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in assessing text readability. Your task is to analyze the given passage and predict its readability score on a continuous scale. The readability score reflects how easy or difficult the text is to understand. \n",
      "\n",
      "    ### Passage:\n",
      "    On the way Mother Mitchel arranged in her head the plan of the monument which was to immortalize her, and considered the means of executing it. As to its form and size, it was to be as exact a copy of the capitol as possible, since the King had willed it; but its outside crust should have a beauty all its own. The dome must be adorned with sugarplums of all colors, and surmounted by a splendid crown of macaroons, spun sugar, chocolate, and candied fruits. It was no small affair.\n",
      "Mother Mitchel did not like to lose her time. Her plan of battle once formed, she recruited on her way all the little pastry cooks of the country, as well as all the tiny six-year-olds who had a sincere love for the noble callings of scullion and apprentice. There were plenty of these, as you may suppose, in the country of the Greedy; Mother Mitchel had her pick of them.\n",
      "\n",
      "    ### Response: The predicted readability score for the passage is  -1.101295212 -1.101295212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585e68c2d0244b52a45b4eceb7d4181a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1372' max='4725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1372/4725 4:17:30 < 10:30:15, 0.09 it/s, Epoch 7.25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Rmse</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>R2</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Spearman's rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.771700</td>\n",
       "      <td>1.584139</td>\n",
       "      <td>0.822549</td>\n",
       "      <td>0.997381</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.236474</td>\n",
       "      <td>0.262639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.560200</td>\n",
       "      <td>1.574396</td>\n",
       "      <td>0.707961</td>\n",
       "      <td>0.760947</td>\n",
       "      <td>0.872323</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.239731</td>\n",
       "      <td>0.697647</td>\n",
       "      <td>0.697302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.496900</td>\n",
       "      <td>1.591292</td>\n",
       "      <td>0.592208</td>\n",
       "      <td>0.530775</td>\n",
       "      <td>0.728543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>0.760845</td>\n",
       "      <td>0.774728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.452200</td>\n",
       "      <td>1.663153</td>\n",
       "      <td>0.535018</td>\n",
       "      <td>0.453481</td>\n",
       "      <td>0.673410</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.546923</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>0.799927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.321400</td>\n",
       "      <td>1.661806</td>\n",
       "      <td>0.539787</td>\n",
       "      <td>0.447904</td>\n",
       "      <td>0.669256</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.552495</td>\n",
       "      <td>0.772991</td>\n",
       "      <td>0.785308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.271100</td>\n",
       "      <td>1.805619</td>\n",
       "      <td>0.489031</td>\n",
       "      <td>0.384716</td>\n",
       "      <td>0.620255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.615627</td>\n",
       "      <td>0.797315</td>\n",
       "      <td>0.808364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.129300</td>\n",
       "      <td>1.773914</td>\n",
       "      <td>0.494679</td>\n",
       "      <td>0.393153</td>\n",
       "      <td>0.627019</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.607197</td>\n",
       "      <td>0.797701</td>\n",
       "      <td>0.805109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.051500</td>\n",
       "      <td>1.915941</td>\n",
       "      <td>0.510871</td>\n",
       "      <td>0.412457</td>\n",
       "      <td>0.642228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.587910</td>\n",
       "      <td>0.804620</td>\n",
       "      <td>0.810957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.970200</td>\n",
       "      <td>1.882589</td>\n",
       "      <td>0.524402</td>\n",
       "      <td>0.435921</td>\n",
       "      <td>0.660243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.564467</td>\n",
       "      <td>0.792142</td>\n",
       "      <td>0.799119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>2.108681</td>\n",
       "      <td>0.566026</td>\n",
       "      <td>0.496934</td>\n",
       "      <td>0.704936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503508</td>\n",
       "      <td>0.770530</td>\n",
       "      <td>0.789009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>2.044942</td>\n",
       "      <td>0.475306</td>\n",
       "      <td>0.361972</td>\n",
       "      <td>0.601641</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.638351</td>\n",
       "      <td>0.801086</td>\n",
       "      <td>0.812526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.681600</td>\n",
       "      <td>2.228910</td>\n",
       "      <td>0.460337</td>\n",
       "      <td>0.337053</td>\n",
       "      <td>0.580562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663247</td>\n",
       "      <td>0.816816</td>\n",
       "      <td>0.822115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.659400</td>\n",
       "      <td>2.200631</td>\n",
       "      <td>0.478465</td>\n",
       "      <td>0.349558</td>\n",
       "      <td>0.591234</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.650753</td>\n",
       "      <td>0.807431</td>\n",
       "      <td>0.812987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file = \"commonlitreadabilityprize.csv\"\n",
    "\n",
    "# Load the CSV file into a Hugging Face dataset\n",
    "dataset = load_dataset(\"csv\", data_files=csv_file)\n",
    "\n",
    "\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Access the train and test datasets\n",
    "train_data = train_test_split['train']\n",
    "val_data = train_test_split['test']\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "#from roberta import RobertaForSequenceClassification\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# Log in using your Hugging Face token\n",
    "login(\"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\")\n",
    "\n",
    "model_name ='meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.model_max_length = 1000\n",
    "\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    # Print the data point for debugging (optional)\n",
    "    # print(data_point)\n",
    "    \n",
    "    return f\"\"\"You are an expert in assessing text readability. Your task is to analyze the given passage and predict its readability score on a continuous scale. The readability score reflects how easy or difficult the text is to understand. \n",
    "\n",
    "    ### Passage:\n",
    "    {data_point['excerpt']}\n",
    "\n",
    "    ### Response: The predicted readability score for the passage is \"\"\"\n",
    "\n",
    "# Assuming `dataset` is your DatasetDict\n",
    "def add_label_column(example):\n",
    "    total_length = 19\n",
    "    num = float(example['target'])\n",
    "\n",
    "    #formatted_num = str(num) + tokenizer.pad_token * padding_length\n",
    "\n",
    "    # Add labels and outputs to the example\n",
    "    example['labels'] = float(example['target'])\n",
    "    example['output'] = str(num)\n",
    "    example['input'] = generate_prompt(example)\n",
    "\n",
    "    \n",
    "    return example\n",
    "\n",
    "train_data = train_data.map(add_label_column)\n",
    "val_data = val_data.map(add_label_column)\n",
    "\n",
    "\n",
    "\n",
    "print(train_data['input'][2], train_data['labels'][2], train_data['output'][2])\n",
    "\n",
    "from generator.modeling import PredictorCausalLM\n",
    "from generator.collator import DataCollator\n",
    "from generator import metrics\n",
    "from generator.training import GenTrainer\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)  # Load configuration\n",
    "config.dense_representation = 10 \n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "config.nub_of_token_generation = 59\n",
    "\n",
    "from generator.modeling import CausalLM\n",
    "from generator.collator import DataCollator\n",
    "from generator import metrics\n",
    "from generator.training import GenTrainer\n",
    "model = CausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "import leader\n",
    "\n",
    "leader.PEFT(model, method='column', rank=3)\n",
    "\n",
    "\n",
    "data_collator = DataCollator(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='dir',\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.00,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=10000000,\n",
    "    logging_steps=100,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type=\"cosine\",  # You can choose from 'linear', 'cosine', 'cosine_with_restarts', 'polynomial', etc.\n",
    "    warmup_steps=200,\n",
    ")\n",
    "\n",
    "compute_metrics = metrics.RegressionMetrics(tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #max_steps_for_sampling=500,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383410e-2b16-4a0a-a578-2d50119abb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
